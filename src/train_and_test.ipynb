{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVO1bbAgxNpr"
      },
      "outputs": [],
      "source": [
        "!pip install fasttext\n",
        "!pip install tensorflow_addons\n",
        "!pip install transformers\n",
        "!pip install sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08DfphDbxlJB"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnMp8BdTx-jP"
      },
      "outputs": [],
      "source": [
        "!7za x '/content/drive/MyDrive/Copy of cc.en.100.bin.7z'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KswdOP7_y6cf"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "import string\n",
        "from sklearn.metrics import accuracy_score\n",
        "from itertools import combinations\n",
        "import ast\n",
        "import tensorflow as tf\n",
        "import fasttext\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow_addons as tfa\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import re\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras import backend as K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBwicyFJ-glX"
      },
      "outputs": [],
      "source": [
        "def preprocess(text):\n",
        "  clean_text = text.strip()\n",
        "  clean_text = clean_text.replace(' ','')\n",
        "  clean_text = clean_text.lower()\n",
        "  clean_text = re.sub(r'\\d+','',clean_text)\n",
        "  lookup_table = clean_text.maketrans('', '', string.punctuation)\n",
        "  clean_text = clean_text.translate(lookup_table)\n",
        "  return clean_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yblIcHS_HRE7"
      },
      "outputs": [],
      "source": [
        "def get_metrics(y_test_final,y_test_predicted_clean):\n",
        "  tn, fp, fn, tp = confusion_matrix(y_test_final, y_test_predicted_clean).ravel()\n",
        "  specificity = tn / (tn+fp)\n",
        "  precision = tp / (tp+fp)\n",
        "  recall = tp / (tp+fn)\n",
        "  accuracy = (tn+tp)/(tn+fp+fn+tp)\n",
        "  f1 = 2*(precision*recall) / (precision+recall)\n",
        "  return specificity,precision,recall,accuracy,f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ONltCf7m2Rd"
      },
      "outputs": [],
      "source": [
        "def split(df):\n",
        "  train = df[df['split'].isin(['train'])]\n",
        "  x_train = train[[\"embedding0\",\"embedding1\",\"category_0\",\"category_1\"]]\n",
        "  test = df[df['split'].isin(['test'])]\n",
        "  x_test =  test[[\"embedding0\",\"embedding1\",\"category_0\",\"category_1\"]]\n",
        "  y_train = train[\"label\"]\n",
        "  y_test = test[\"label\"]\n",
        "  return x_train, x_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoA5CC11mOgL"
      },
      "outputs": [],
      "source": [
        "def split_and_train_mlp(df,shape):\n",
        "  x_train, x_test, y_train, y_test = split(df)\n",
        "  np.random.seed(42)\n",
        "  tf.random.set_seed(42)\n",
        "  input1 = tf.keras.layers.Input(shape=(shape,))\n",
        "  input2 = tf.keras.layers.Input(shape=(shape,))\n",
        "  layer = tf.keras.layers.concatenate([input1,input2],axis=1)\n",
        "  layer = tf.keras.layers.Dense(128,activation='relu',kernel_initializer=tf.keras.initializers.GlorotUniform(seed=42))(layer)\n",
        "  layer = tf.keras.layers.Dense(1,activation='sigmoid',kernel_initializer=tf.keras.initializers.GlorotUniform(seed=42))(layer)\n",
        "  model = tf.keras.Model([input1,input2],layer)\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(),loss=tf.keras.losses.BinaryCrossentropy(),metrics='accuracy')\n",
        "  x_train_embedding0 = np.array(x_train['embedding0'].values.tolist()).astype('float32')\n",
        "  x_train_embedding1 = np.array(x_train['embedding1'].values.tolist()).astype('float32')\n",
        "  y_train_final = np.array(y_train.tolist()).astype(int)\n",
        "  x_test_embedding0 = np.array(x_test['embedding0'].values.tolist()).astype('float32')\n",
        "  x_test_embedding1 = np.array(x_test['embedding1'].values.tolist()).astype('float32')\n",
        "  y_test_final = np.array(y_test.tolist()).astype(int)\n",
        "  callback_1 = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
        "  model.fit(x=[x_train_embedding0,x_train_embedding1],y=y_train_final,epochs=20,validation_split=0.2,callbacks=[callback_1],shuffle=True)\n",
        "  y_test_predicted = model.predict(x=[x_test_embedding0,x_test_embedding1])\n",
        "  y_test_predicted_clean = [round(pred[0]) for pred in y_test_predicted]\n",
        "  specificity,precision,recall,accuracy,f1 = get_metrics(y_test_final,y_test_predicted_clean)\n",
        "  print(\"Specificity: {}, Precision: {}, Recall {}, Accuracy {},F1 Score {}\".format(specificity,precision,recall,accuracy,f1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNjf4yEHuST_"
      },
      "outputs": [],
      "source": [
        "def split_and_train_lstm(df):\n",
        "  x_train, x_test, y_train, y_test = split(df)\n",
        "  np.random.seed(42)\n",
        "  tf.random.set_seed(42)\n",
        "  cat0 = x_train['category_0'].tolist()\n",
        "  cat1 = x_train['category_1'].tolist()\n",
        "  testcat0 = x_test['category_0'].tolist()\n",
        "  testcat1= x_test['category_1'].tolist()\n",
        "  chars = set()\n",
        "  for c in cat0:\n",
        "    for x in c:\n",
        "      chars.add(x)\n",
        "  for c in cat1:\n",
        "    for x in c:\n",
        "      chars.add(x)\n",
        "  chars = list(chars)\n",
        "  char_to_id = {c:i for i,c in enumerate(chars)}\n",
        "  id_to_char = {i:c for i,c in enumerate(chars)}\n",
        "  x_train_emb1 = []\n",
        "  x_train_emb2 = []\n",
        "  x_test_emb1 = []\n",
        "  x_test_emb2 = []\n",
        "\n",
        "  for cat in cat0:\n",
        "    curr_emb = [char_to_id[c] for c in cat]\n",
        "    x_train_emb1.append(curr_emb)\n",
        "\n",
        "  for cat in cat1:\n",
        "    curr_emb = [char_to_id[c] for c in cat]\n",
        "    x_train_emb2.append(curr_emb)\n",
        "\n",
        "  for cat in testcat0:\n",
        "    curr_emb = [char_to_id[c] for c in cat]\n",
        "    x_test_emb1.append(curr_emb)\n",
        "\n",
        "  for cat in testcat1:\n",
        "    curr_emb = [char_to_id[c] for c in cat]\n",
        "    x_test_emb2.append(curr_emb)\n",
        "\n",
        "  x_train_emb1 = tf.keras.utils.pad_sequences(\n",
        "  x_train_emb1,\n",
        "  maxlen=100,\n",
        "  dtype='int32',\n",
        "  padding='pre',\n",
        "  truncating='pre',\n",
        "  value=0.0)\n",
        "  x_train_emb2 = tf.keras.utils.pad_sequences(\n",
        "      x_train_emb2,\n",
        "      maxlen=100,\n",
        "      dtype='int32',\n",
        "      padding='pre',\n",
        "      truncating='pre',\n",
        "      value=0.0)\n",
        "  x_test_emb1 = tf.keras.utils.pad_sequences(\n",
        "      x_test_emb1,\n",
        "      maxlen=100,\n",
        "      dtype='int32',\n",
        "      padding='pre',\n",
        "      truncating='pre',\n",
        "      value=0.0)\n",
        "  x_test_emb2 = tf.keras.utils.pad_sequences(\n",
        "      x_test_emb2,\n",
        "      maxlen=100,\n",
        "      dtype='int32',\n",
        "      padding='pre',\n",
        "      truncating='pre',\n",
        "      value=0.0)\n",
        "  MAX_LEN = 100\n",
        "  callback_1 = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
        "  print(\"Training LSTM......\")\n",
        "  first_sent_in = tf.keras.layers.Input(shape=(MAX_LEN,))\n",
        "  second_sent_in = tf.keras.layers.Input(shape=(MAX_LEN,))\n",
        "  embedding_layer =  tf.keras.layers.Embedding(len(char_to_id)+1,100, input_length=MAX_LEN)\n",
        "  first_sent_embedding = embedding_layer(first_sent_in)\n",
        "  second_sent_embedding = embedding_layer(second_sent_in)\n",
        "  lstm =  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=256, return_sequences=False,kernel_initializer=tf.keras.initializers.GlorotUniform(seed=42)))\n",
        "  first_sent_encoded = lstm(first_sent_embedding)\n",
        "  second_sent_encoded = lstm(second_sent_embedding)\n",
        "  l1_norm = lambda x: 1 - K.abs(x[0] - x[1])\n",
        "  merged = tf.keras.layers.Lambda(function=l1_norm, output_shape=lambda x: x[0], name='L1_distance')([first_sent_encoded, second_sent_encoded])\n",
        "  predictions = tf.keras.layers.Dense(1, activation='sigmoid', name='classification_layer',kernel_initializer=tf.keras.initializers.GlorotUniform(seed=42))(merged)\n",
        "  model = tf.keras.Model([first_sent_in, second_sent_in], predictions)\n",
        "  model.compile(loss = 'binary_crossentropy', optimizer = \"adam\", metrics=[\"accuracy\"])\n",
        "  print(model.summary())\n",
        "  model.fit([x_train_emb1, x_train_emb2], y_train.to_numpy().astype(int), validation_split=0.1, epochs = 20,shuffle=True, batch_size = 512,callbacks=[callback_1])\n",
        "  y_test_predicted = model.predict(x=[x_test_emb1,x_test_emb2])\n",
        "  y_test_predicted_clean = [round(pred[0]) for pred in y_test_predicted]\n",
        "  specificity,precision,recall,accuracy,f1 = get_metrics(y_test.to_numpy().astype(int),y_test_predicted_clean)\n",
        "  print(\"Specificity: {}, Precision: {}, Recall {}, Accuracy {},F1 Score {}\".format(specificity,precision,recall,accuracy,f1))\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKKme2phnEUg"
      },
      "outputs": [],
      "source": [
        "# ft = fasttext.load_model('/content/cc.en.100.bin')\n",
        "df = pd.read_csv(\"/content/ood_pairs_simple3_86.csv\")\n",
        "df = df[[\"category_0\",\"category_1\",\"label\",\"split\"]]\n",
        "df[\"category_0\"] = df[\"category_0\"].map(preprocess)\n",
        "df[\"category_1\"] = df[\"category_1\"].map(preprocess)\n",
        "df = df.sample(frac=1,random_state=42).reset_index() #shuffle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-URR2ifp5HS"
      },
      "outputs": [],
      "source": [
        "xf = pd.read_csv(\"synthetic_data_final.csv\")\n",
        "xf.rename(columns={'cat0':'category_0','cat1':'category_1'},inplace=True)\n",
        "xf.dropna(inplace=True)\n",
        "xf[\"split\"] = \"train\"\n",
        "xf[\"category_0\"] = xf[\"category_0\"].map(preprocess)\n",
        "xf[\"category_1\"] = xf[\"category_1\"].map(preprocess)\n",
        "combined_dataset = pd.concat([xf,df])\n",
        "combined_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRm0pEyCItR-"
      },
      "source": [
        "# Experiments with OOD data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTNApgyZykto"
      },
      "outputs": [],
      "source": [
        "df['embedding0'] = df['category_0'].apply(lambda x:ft.get_word_vector(x).tolist())\n",
        "df['embedding1'] = df['category_1'].apply(lambda x:ft.get_word_vector(x).tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TGFR0Lx6_e5"
      },
      "outputs": [],
      "source": [
        "df['embedding0'] = 0\n",
        "df['embedding1'] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzpxY50vysnG"
      },
      "outputs": [],
      "source": [
        "split_and_train_mlp(df,100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qN6SFwX4ndmI"
      },
      "outputs": [],
      "source": [
        "llm = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "bert_embeddings0 =  llm.encode(df['category_0'].tolist())\n",
        "bert_embeddings1 =  llm.encode(df['category_1'].tolist())\n",
        "df['embedding0'] = bert_embeddings0.tolist()\n",
        "df['embedding1'] = bert_embeddings1.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFI_O8dfn3zu"
      },
      "outputs": [],
      "source": [
        "split_and_train_mlp(df,384)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWcG8t4Pt6U3"
      },
      "outputs": [],
      "source": [
        "split_and_train_lstm(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhGuDKfJ2R4z"
      },
      "source": [
        "# Experiments with Synthetic Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5O7aDN7-qmyG"
      },
      "outputs": [],
      "source": [
        "combined_dataset['embedding0'] = combined_dataset['category_0'].apply(lambda x:ft.get_word_vector(x).tolist())\n",
        "combined_dataset['embedding1'] = combined_dataset['category_1'].apply(lambda x:ft.get_word_vector(x).tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSPO9iKm7DZX"
      },
      "outputs": [],
      "source": [
        "combined_dataset['embedding0'] = 0\n",
        "combined_dataset['embedding1'] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLsclCTAq0ih"
      },
      "outputs": [],
      "source": [
        "split_and_train_mlp(combined_dataset,100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTOrb0ylt5Wd"
      },
      "outputs": [],
      "source": [
        "llm = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "bert_embeddings0 =  llm.encode(combined_dataset['category_0'].tolist())\n",
        "bert_embeddings1 =  llm.encode(combined_dataset['category_1'].tolist())\n",
        "combined_dataset['embedding0'] = bert_embeddings0.tolist()\n",
        "combined_dataset['embedding1'] = bert_embeddings1.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EcQPbS-It6_O"
      },
      "outputs": [],
      "source": [
        "split_and_train_mlp(combined_dataset,384)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "B7TQNCljAs3p",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "model = split_and_train_lstm(combined_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtVVdyd5lvYD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}